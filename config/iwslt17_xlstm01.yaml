# config/iwslt17_xlstm01.yaml

training:
  seed: 1337
  batch_size: 16
  num_epochs: 3
  lr: 0.0002
  weight_decay: 0.01
  grad_clip: 1.0
  patience_epochs: 3
  num_workers: 4

dataset:
  vocab_size: 8000
  max_src_len: 128
  max_tgt_len: 128

model:
  num_blocks: 4
  embedding_dim: 256

  mlstm_block:
    mlstm:
      conv1d_kernel_size: 4
      qkv_proj_blocksize: 4
      num_heads: 4

  # full slstm config
  slstm_block:
    slstm:
      backend: cuda
      num_heads: 4
      conv1d_kernel_size: 4
      bias_init: powerlaw_blockdependent
    feedforward:
      proj_factor: 1.3
      act_fn: gelu

  # use sLSTM in every block
  slstm_at: [0, 1, 2, 3]

  context_length: 128
